\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{tcolorbox}

\usetheme{metropolis} % Tema moderno e minimale


\title{Identificazione e disambiguazione della costruzione NPN con BERT}
\subtitle{Caso studio di Scivetti e Schneider sulla costruzione inglese} % <-- sottotitolo
\author{Corso di Semantica a.a. 2025/2026}
\institute{Università di Bologna}
\date{18 Novembre 2025}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Indice}
  \tableofcontents
\end{frame}

\section{La costruzione NPN}

\begin{frame}{Cos'è la costruzione NPN}
  La costruzione NPN è stata ampiamente studiata in inglese da una prospettiva costruzionista,
  in particolare da Jackendoff (2008) e Sommerer \& Baumann (2021).

  \begin{tcolorbox}[colback=white, colframe=black, arc=4mm, boxrule=0.8pt, center title, title=Construction Schema]
    \[
      Noun_1 \; \text{Preposition} \; Noun_2
    \]
  \end{tcolorbox}

  I due nomi nella costruzione devono avere forma identica, anche per la declinazione del numero.
  La costruzione non ammette nomi accompagnati da determinati.
\end{frame}

\begin{frame}{Cos'è la costruzione NPN}
  La costruzione può comparire in diverse posizioni sintattiche, ad esempio come modificatore avverbiale o come modificatore nominale.

  \begin{exampleblock}{Esempi}
    \begin{itemize}
      \item \textit{I need you to get this \textbf{word for word.}} 
      \\
      \hfill (modificatore avverbiale)
      \item \textit{There is a rebellious quality to your \textbf{day to day}
responses which have not gone unnoticed.} \hfill (modificatore nominale)
    \end{itemize}
  \end{exampleblock}

\end{frame}

\begin{frame}{Significato e funzione}

Significati delle costruzioni NPN istanziate da \textit{to}

  \begin{block}{SUCCESSION}
    Testo del blocco esplicativo.
  \end{block}

  \begin{block}{JUXTAPOSITION}
    Testo del blocco esplicativo.
  \end{block}

\end{frame}


\section{Il Dataset}

\begin{frame}{Il Dataset}
  \begin{itemize}
    \item estrazione da COCA
    \item Eliminazione dei casi PNPN
    \item Identificazione dei distrattori
    \item Annotazione per tutte le istanze delle etichette semantiche 
  \end{itemize}
\end{frame}

\begin{frame}{Affidabilità dell'annotazione}

\textbf{Doppia annotazione}
\begin{itemize}
    \item Annotato \textbf{~25\%} del dataset
    \item Accordo grezzo: \textbf{84\%}
    \item Cohen's kappa: \textbf{0.754} (accordo forte)
\end{itemize}

\vspace{0.5cm}

\textbf{Dimensioni del dataset}
\begin{itemize}
    \item \textbf{6599} istanze totali (N-to-N)
    \item \textbf{1885} istanze con doppia annotazione
\end{itemize}

\end{frame}

\section{BERT, modello di encoder}

\begin{frame}{Embedding e trasformazioni del testo}

\begin{block}{Embedding}
Le parole e le frasi vengono trasformate in \textbf{vettori numerici} chiamati embedding. 
Questi vettori catturano somiglianze semantiche e relazioni tra parole, permettendo al modello di “comprendere” il testo.
\end{block}

\begin{block}{Perché trasformare il testo}
Il testo deve diventare numerico per essere elaborato dai modelli. 
Trasformare significa codificare ogni parola in uno spazio continuo dove vicinanza = somiglianza.
\end{block}

\end{frame}

\begin{frame}{Transformer e Encoder}

\begin{block}{Transformer}
Modello basato su \textbf{self-attention}: ogni parola osserva tutte le altre per capire il contesto. 
Non usa ricorrenza e permette di gestire sequenze lunghe in parallelo.
\end{block}

\begin{block}{Encoder}
L’encoder produce rappresentazioni contestuali di ogni parola, che riflettono sia il significato intrinseco sia le relazioni con le altre parole nel testo.
\end{block}

\end{frame}

\begin{frame}{BERT: bidirezionale e contestuale}

\begin{block}{Cos'è BERT}
BERT è un Transformer \textbf{solo encoder}, che legge il contesto a sinistra e a destra di ogni parola.
Produce embedding contestuali che catturano significato, struttura sintattica e relazioni tra parole.
\end{block}

\begin{block}{Addestramento}
- \textbf{Masked Language Modeling}: predire token mascherati.\\
- \textbf{Next Sentence Prediction}: capire se due frasi sono in sequenza.
\end{block}

\begin{block}{Perché usarlo}
Le rappresentazioni di BERT possono essere adattate a molti task NLP: classificazione, NER, question answering. 
Per questo caso studio, mostrano come la semantica delle costruzioni è rappresentata.
\end{block}

\end{frame}

\section{Tadsk 1: Identificazione}

\begin{frame}{Dataset}
  \begin{block}{Esempio di blocco}
    Testo del blocco esplicativo.
  \end{block}
\end{frame}

\section{Task 2: Identificazione (perturbando l'ordine delle parole)}

\begin{frame}{Paper S}
  \begin{block}{Esempio di blocco}
    Testo del blocco esplicativo.
  \end{block}
\end{frame}

\section{Task 3: Disambiguazione semantica}

\begin{frame}{Paper S}
  \begin{block}{Esempio di blocco}
    Testo del blocco esplicativo.
  \end{block}
\end{frame}

\end{document}
