\documentclass{beamer}

% Per grafica, immagini, figure
\usepackage{graphicx}

% Per simboli matematici, frecce, ecc.
\usepackage{amsmath}
\usepackage{amssymb}

% Per gestione delle colonne
\usepackage{multicol}

% Per migliorare lo spacing
\usepackage{ragged2e}



\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{tikz}           % Per disegnare blocchi, frecce, diagrammi
\usetikzlibrary{positioning,fit,arrows.meta}

\usetheme{metropolis} % Tema moderno e minimale


\title{Identificazione e disambiguazione della costruzione NPN con BERT}
\subtitle{Caso studio di Scivetti e Schneider sulla costruzione inglese} % <-- sottotitolo
\author{Corso di Semantica a.a. 2025/2026}
\institute{Università di Bologna}
\date{18 Novembre 2025}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Indice}
  \tableofcontents
\end{frame}

\section{La costruzione NPN}

\begin{frame}{Cos'è la costruzione NPN}
  La costruzione NPN è stata ampiamente studiata in inglese da una prospettiva costruzionista,
  in particolare da Jackendoff (2008) e Sommerer \& Baumann (2021).

  \begin{tcolorbox}[colback=white, colframe=black, arc=4mm, boxrule=0.8pt, center title, title=Construction Schema]
    \[
      Noun_1 \; \text{Preposition} \; Noun_2
    \]
  \end{tcolorbox}

  I due nomi nella costruzione devono avere forma identica, anche per la declinazione del numero.
  La costruzione non ammette nomi accompagnati da determinati.
\end{frame}

\begin{frame}{Cos'è la costruzione NPN}
  La costruzione può comparire in diverse posizioni sintattiche, ad esempio come modificatore avverbiale o come modificatore nominale.

  \begin{exampleblock}{Esempi}
    \begin{itemize}
      \item \textit{I need you to get this \textbf{word for word.}} 
      \\
      \hfill (modificatore avverbiale)
      \item \textit{There is a rebellious quality to your \textbf{day to day}
responses which have not gone unnoticed.} \hfill (modificatore nominale)
    \end{itemize}
  \end{exampleblock}

\end{frame}

\begin{frame}{Significato e funzione}

Significati delle costruzioni NPN istanziate da \textit{to}

  \begin{block}{SUCCESSION}
    Testo del blocco esplicativo.
  \end{block}

  \begin{block}{JUXTAPOSITION}
    Testo del blocco esplicativo.
  \end{block}

\end{frame}


\begin{frame}{Obbietivo dello studio}

Valutare se i LLMs sono in grado di riconoscere l'unità fondamentale della Construction Grammar: la costruzione.

\end{frame}


\section{Accedere alla conoscenza di un LLM}
\begin{frame}{LLMs come Black Box}


I Large Language Models ricevono input e producono output, \\[2pt]
\textbf{ma il loro funzionamento interno rimane in gran parte opaco}.


\vspace{4mm}

\begin{itemize}
    \item \textbf{Non osservabile}: non possiamo seguire direttamente come le reti trasformano l’informazione.
    \item \textbf{Non interpretabile}: le rappresentazioni interne sono vettori numerici di grandi dimensioni.
\end{itemize}

\vspace{4mm}

\begin{alertblock}{Domande chiave}
\begin{itemize}
    \item Quali \textbf{rappresentazioni interne} costruiscono?
    \item Quali \textbf{pattern linguistici} apprendono?
    \item In che modo \textbf{utilizzano} queste conoscenze nei task?
\end{itemize}
\end{alertblock}

\end{frame}

\begin{frame}{Struttura del Transformer}

\begin{center}
\textbf{Self-Attention} \\[2mm]
\small Consente al modello di pesare dinamicamente tutte le parti dell’input.
\end{center}

\vspace{4mm}

\begin{columns}[t]

% ENCODER
\column{0.45\textwidth}
\begin{block}{\centering \large Encoder}
\begin{itemize}
    \item Converte il testo di input in
    \textbf{rappresentazioni vettoriali contestuali}.
\end{itemize}
\end{block}

\column{0.10\textwidth}
\centering
\Huge $\Rightarrow$

% DECODER
\column{0.45\textwidth}
\begin{block}{\centering \large Decoder}
\begin{itemize}
    \item Genera testo \textbf{un token alla volta}.
\end{itemize}
\end{block}

\end{columns}

\vspace{4mm}

\begin{alertblock}{Tipologie di modelli Transformer}
\textbf{Encoder-only}: BERT, RoBERTa 
\textbf{Decoder-only}: GPT, LLaMA 
\textbf{Encoder–decoder}: T5, BART
\end{alertblock}


\end{frame}


\begin{frame}{BERT: modello encoder-only}

\begin{block}{BERT e la costruzione NPN}
Il case study del paper indaga la conoscenza che BERT possiede
della costruzione \textbf{NPN} (Noun–Prep–Noun).
\end{block}

\vspace{4mm}

\begin{block}{Obiettivi di pre-training}
BERT viene addestrato su due task fondamentali:
\begin{itemize}
    \item \textbf{Masked Language Modeling (MLM)}: si maschera un token e il modello deve
          \textbf{ricostruirlo a partire dal contesto}.
    \item \textbf{Next Sentence Prediction (NSP)}: il modello deve
          \textbf{capire se due frasi sono consecutive} nel testo originale.
\end{itemize}
\end{block}

\vspace{4mm}


\end{frame}

\begin{frame}{Punto chiave: rappresentazioni profonde}
\begin{columns}[t] % allinea in alto
  % Colonna testo
  \column{0.5\textwidth}
  \begin{alertblock}{Punto chiave}
  Lo scopo non è risolvere i task in sé,  
  ma \textbf{insegnare al modello a costruire rappresentazioni linguistiche profonde},  
  cioè \textbf{vettori contestuali} ricchi di informazione sintattica e semantica.
  \end{alertblock}

  % Colonna grafica
  \column{0.5\textwidth}
  \begin{center}
    \includegraphics[width=1.1\textwidth]{images/MLM.png} % icona o diagramma
    \\[2mm]
    \small Masked language modeling
  \end{center}
\end{columns}
\end{frame}


\begin{frame}{Che cos'è un vettore?}
\centering
\begin{tikzpicture}[node distance=0.6cm, every node/.style={align=center, font=\small}]
  % Blocchi principali
  \node (token) [draw, rounded corners, fill=blue!40, minimum width=3cm, minimum height=1cm, thick] {Token};
  
  \node (vector) [draw, rounded corners, fill=green!40, below=of token, minimum width=3cm, minimum height=1cm, thick] {Trasformazione in vettore \\ (insieme di numeri)};
  
  \node (context) [draw, rounded corners, fill=yellow!40, below=of vector, minimum width=3cm, minimum height=1cm, thick] {Definito distribuzionalmente \\ dalle parole nel contesto};
  
  % Blocchi diramati
  \node (relation) [draw, rounded corners, fill=red!60, text=white, minimum width=2.5cm, minimum height=1.5cm, below left=0.5cm and -2cm of context, thick] {Rappresentazione cattura \\ relazioni semantiche e sintattiche};
  
  \node (space) [draw, rounded corners, fill=orange!60, text=white, minimum width=2.5cm, minimum height=1.5cm, below right=0.5cm and -2.25cm of context, thick] {Spazio n-dimensionale \\ (n dimensioni = dimensione del vettore)};
  
  % Frecce
  \draw[->, thick] (token) -- (vector);
  \draw[->, thick] (vector) -- (context);
  \draw[->, thick] (context) -- (relation);
  \draw[->, thick] (context) -- (space);
\end{tikzpicture}
\end{frame}

\begin{frame}{Rappresentazione vettoriale}
\begin{center}
    \includegraphics[width=1\textwidth]{images/static.png}
    \\[2mm]
    \small Embedding statici
\end{center}
\end{frame}

\begin{frame}{Rappresentazione vettoriale in BERT}

Fasi principali:

\begin{enumerate}
    \item \textbf{Tokenizzazione:} ogni token viene convertito in un vettore iniziale.

    \item \textbf{Previsione [MASK]:} cattura informazione linguistica contestuale, propagata attraverso i layer.

    % Riquadro vettore layer
    \begin{center}
    \begin{tcolorbox}[colback=green!10, colframe=green!50!black, rounded corners, boxrule=0.8pt, width=1\textwidth]
        Layer dopo layer, il vettore acquisisce informazioni sempre più accurate.
    \end{tcolorbox}
    \end{center}

    \item \textbf{Aggiornamento pesi:} predizione su [MASK] \\
    Confronto con token corretto (LOSS)
    \\ 
    Aggiustamento dei parametri (BACKPROPAGATION).

    % Riquadro vettori finali
\begin{center}
\begin{tcolorbox}[
    colback=red!10, 
    colframe=red!50!black, 
    rounded corners, 
    boxrule=0.8pt, 
    width=0.95\textwidth, % rimpicciolisce la larghezza
    boxsep=2mm,          % padding interno ridotto
    fontupper=\small      % testo più piccolo
]
    Vettori numerici (768d) risultanti dall'addestramento di BERT
\end{tcolorbox}
\end{center}

    \item Risoluzione del task: i vettori contestuali possono essere usati per classificazione, NER, question answering, ecc.
\end{enumerate}

\end{frame}

\begin{frame}{BERT: embedding contestuali}

\begin{center}
\begin{tcolorbox}[
    colback=blue!10, 
    colframe=blue!50!black, 
    rounded corners, 
    boxrule=1pt, 
    width=0.85\textwidth, 
    boxsep=1.5mm,
    fontupper=\small
]
La risoluzione del task \textbf{Masked Language Modeling (MLM)} disegna lo \textbf{spazio vettoriale}.
\end{tcolorbox}
\end{center}

\vspace{3mm}

\begin{itemize}
    \item \textbf{Rappresentazione contestuale}: ogni token ha un vettore diverso a seconda della frase in cui compare.
    \item \textbf{Contesto bidirezionale}: il modello vede tutti i token della frase e aggiorna gli embedding a ogni layer.
\end{itemize}

\end{frame}

\section{Probing}

\begin{frame}{Probing: accesso alle informazioni linguistiche}
Vettori numerici complessi $\rightarrow$ non interpretabili

\begin{block}{Probing}
  Interrogare i \textbf{vettori} per verificare se contengono determinate conoscenze linguistiche
\end{block}

\begin{tikzpicture}
  % Draw a rectangle from (0,0) to (4,2)
  \node (rect1) [draw, rectangle, rounded corners, fill=blue!20, minimum width=2cm, minimum height=1cm, align=center] 
       {Estrarre embedding\\dal modello};

  % Second rectangle below the first
  \node (rect2) [draw, rectangle, rounded corners, fill=green!20, minimum width=2cm, minimum height=1cm, align=center, below=of rect1] 
       {Dataset\\annotato};

  \draw[thick] (0,-1.1) circle (2.4cm);

  \draw[->, thick] (2.7,-1.1) -- (4.2, -1.1);

  \node (class) [draw, rounded corners, minimum width=2cm, minimum height=1cm, align=center] 
       at (5.4,-1.1) {\footnotesize Classificatore\\\footnotesize semplice};

  % Optional: add a label at the center
  \node at (0,-1.1) {$+$};
  

  \draw[->, thick] (6.5, -0.5) -- (7,0.3) node[above right] {LR};
  \draw[->, thick] (6.5, -1.7) -- (7,-2.5) node[below right] {MLP};

\end{tikzpicture}
\end{frame}


\begin{frame}{Differenze rispetto al fine-tuning}
\centering
\begin{tikzpicture}[every node/.style={align=center}]
  % Box principale
  \node (head) [draw=blue!50!black, fill=blue!10, rounded corners, minimum width=8cm, minimum height=1cm] 
    {$\neq$ Classification head nel fine-tuning};

  % Box secondari sotto
  \node (left) [draw, rounded corners, fill=green!10, minimum width=3.5cm, minimum height=1cm, below left=2cm and -2cm of head] {Obbiettivo migliorare \\ le performance non \\
  nterrogare il modelo};
  \node (right) [draw, rounded corners, fill=green!10, minimum width=3.5cm, minimum height=1cm, below right=2cm and -2cm of head] {Modifica i pesi \\ BACKPROPAGATION};

  % Frecce che collegano il box principale a quelli sotto
  \draw[->, thick] (head.south) -- (left.north);
  \draw[->, thick] (head.south) -- (right.north);
\end{tikzpicture}
\end{frame}


\begin{frame}{Interpretare le performance del classificatore}
Per concludere che le buone performance del classificatore riflettono realmente l’informazione linguistica contenuta nei vettori, è necessario:  

\begin{itemize}
  \item Analizzare le performance degli \textbf{embedding a tutti i layer} del modello. $\rightarrow$ Ci attendiamo incremento e differenza.
  \item Stabilire una \textbf{baseline significativa} per confronto.
  \item Confrontare i risultati con un \textbf{control classifier}.
\end{itemize}

\end{frame}







\section{Il Dataset}

\begin{frame}{Il Dataset}
  \begin{itemize}
    \item estrazione da COCA
    \item Eliminazione dei casi PNPN
    \item Identificazione dei distrattori
    \item Annotazione per tutte le istanze delle etichette semantiche 
  \end{itemize}
\end{frame}

\begin{frame}{Affidabilità dell'annotazione}

\textbf{Doppia annotazione}
\begin{itemize}
    \item Annotato \textbf{~25\%} del dataset
    \item Accordo grezzo: \textbf{84\%}
    \item Cohen's kappa: \textbf{0.754} (accordo forte)
\end{itemize}

\vspace{0.5cm}

\textbf{Dimensioni del dataset}
\begin{itemize}
    \item \textbf{6599} istanze totali (N-to-N)
    \item \textbf{1885} istanze con doppia annotazione
\end{itemize}

\end{frame}

\section{Training e Test set}

\begin{frame}{Near Minimal Pairs e Distrattori}

\begin{block}{NtoN distractors}
Oltre alle reali istanze della costruzione NtoN, il corpus contiene anche
pattern superficiali \textbf{Noun + to + Noun} che non sono costruzioni NtoN.
Derivano da contesti sintattici diversi (es. verbi che reggono un oggetto
e una PP con \emph{to}): \emph{stick plastic to plastic}, \emph{time to time travel}, ecc.

Questi casi non esprimono il significato della costruzione ma forniscono utili \textbf{esempi negativi} per testare se il modello.
\end{block}

\begin{block}{Near minimal pairs}
Poiché condividono la stessa forma superficiale dei veri NtoN,
questi distrattori costituiscono \textbf{near minimal pairs}:
frasi grammaticali, naturali, quasi identiche in superficie,
ma con \textbf{struttura e significato diversi}.
\end{block}

\begin{block}{Dataset}
Nel case study \textbf{456} near minimal pairs
come distrattori dal corpus.
\end{block}

\end{frame}

\begin{frame}{Split training test set}
\begin{block}{Evitare overfitting}
Max 20 occorrenze per lemma per evitare overfitting e ridurre la sproporzione tra lemmi altamente frequenti e lemmi rari.
\end{block}

\begin{block}{Controllo della generalizzazione}
Generazione split casuali di train/test basati sul lemma del nome presente nella costruzione NtoN, in modo che nessun lemma compaia sia nel training set sia nel test set.
\end{block}


\begin{block}{Bilancimento training}
Poiché il numero di distrattori è significativamente inferiore rispetto alle istanze della costruzione, per bilanciare le categorie nel training set è stato utilizzato l’80\% dei distrattori, abbinato allo stesso numero di costruzioni. Il test set è quindi composto dal restante 20\% dei distrattori, insieme a tutte le costruzioni eccedenti quelle usate per il training.
\end{block}

\end{frame}


\section{Task 1: Identificazione}

\begin{frame}{Task 1: Identificazione}
  \begin{block}{Definizione del task}
  Distinguere le istanze autentiche della costruzione NtoN dagli esempi autentici del corrispondente pattern distrattore.
  \end{block}
\end{frame}

\begin{frame}{Base-line}
  \begin{block}{Control classifier}
Le etichette vengono randomizzate e assegnate in modo deterministico al word type.

Le performance dovrebbero attestrarsi near chance.
  \end{block}
  \begin{block}{Non-contextual baseline (GloVe)}
Valuta la performance basata solo su informazioni lessicali, senza contesto.

Dovrebbero attndersi, in virtù dei campi semnatici ricorrenti nella costruzione NtoN come espressioni temporali e parti del corpo, performance non trascurabili.
  \end{block}
\end{frame}

\begin{frame}{A cosa serve la baseline?}
  \begin{block}{Control classifier}
Informa sullla bontà del classificatore
  \end{block}
  \begin{block}{Non-contextual baseline (GloVe)}
  Tutto ciò che supera questa performance potrebbe essere attribuito alle informazioni aggiuntive catturate da BERT attraverso il
  significato contestuale.
  \end{block}
\end{frame}

\section{Task 2: Identificazione (perturbando l'ordine delle parole)}


\begin{frame}{Task 2: Perturbing Word Order}
\textbf{Obiettivo}
\begin{itemize}
    \item Testare la \textbf{robustezza} del classificatore BERT-based.
    \item Verificare se distingue la \textbf{vera costruzione NtoN} da
    frasi con ordine delle parole \textbf{alterato artificialmente}.
\end{itemize}

\textbf{Idea di base}
\begin{itemize}
    \item Se il modello si basa troppo su \textbf{indizi lessicali},
    classificherà come positive anche frasi non-NtoN con gli stessi nomi.
    \item Se è sensibile al \textbf{pattern N + to + N}, occorrenze perturbate da reali istanze della costrzione.
\end{itemize}

\end{frame}

\begin{frame}{Task 2: Perturbing Word Order}

\textbf{Metodo}
\begin{itemize}
    \item Non viene riaddestrato il probe: vede solo N + to + N corretti.
    \item Viene manipolato il test set creando 4 ordini delle parole perturbati.
\end{itemize}

\textbf{Tipi di perturbazioni}
\begin{itemize}
    \item \textbf{PNN}: to + N + N
    \item \textbf{PN}: to + N
    \item \textbf{NP}: N + to
    \item \textbf{NNP}: N + N + to
\end{itemize}

\textbf{Scopo finale}
\begin{itemize}
    \item Valutare se il classificatore è sensibile alla \textbf{forma} della costruzione.
\end{itemize}

\end{frame}

\section{Task 3: Disambiguazione semantica}

\begin{frame}{Task 3: Disambiguazione semantica}
\textbf{Obiettivo:} Analizzare i sottotipi semantici della costruzione NtoN.

\begin{itemize}
    \item La performance del classificatore è alta nel distinguere NtoN da pattern simili.
    \item La costruzione NtoN è \textbf{ambigua} e può avere significati diversi a seconda del contesto.
    \item Due significati principali:
    \begin{itemize}
        \item \textbf{SUCCESSIONE} 
        \item \textbf{GIUSTAPPOSIZIONE}
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Task 3: Disambiguazione semantica}
\textbf{Caratteristiche dei sottotipi:}

\begin{itemize}
    \item \textbf{SUCCESSIONE}: frequente con nomi spaziotemporali (es. giorno per giorno, costa a costa)
    \item \textbf{GIUSTAPPOSIZIONE}: frequente con parti del corpo o esseri umani (es. faccia a faccia, amico ad amico)
    \item Il significato del sostantivo non è determinante: alcune occorrenze assumono il significato meno comune a seconda del contesto.
    \item Esistono sostantivi rari per cui non è chiaro quale sottotipo sia più comune.
\end{itemize}
\end{frame}

\begin{frame}{Task 3: Disambiguazione semantica}
\textbf{Metodologia:}

\begin{itemize}
    \item Classificatore per distinguere i sottotipi semantici: 
        \begin{itemize}
            \item SUCCESSIONE
            \item GIUSTAPPOSIZIONE
            \item Non-esempi (pattern distrattori)
        \end{itemize}
    \item Classificazione a 3 classi.
    \item Classificatori di controllo: etichette casuali assegnate a ciascun lemma (Hewitt \& Liang, 2019)
    \item Se le probe sono selettive, i classificatori di controllo dovrebbero ottenere ~33\% di accuratezza.
\end{itemize}
\end{frame}


\end{document}
