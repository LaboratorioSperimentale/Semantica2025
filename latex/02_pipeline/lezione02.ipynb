{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cfd8b8",
   "metadata": {},
   "source": [
    "## Obiettivo della presentazione\n",
    "\n",
    "- Ricostruire la pipeline di ricerca di Scivetti \\& Schneider\n",
    "- consultare il loro codice e identificare i punti salienti della pipeline\n",
    "- riprodurre i risultati\n",
    "- definire i punti critici nell'ottica di integrare dati sull'italiano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393668f2",
   "metadata": {},
   "source": [
    "## La pubblicazione\n",
    "\n",
    "[Link](https://aclanthology.org/2025.conll-1.24/)\n",
    "\n",
    "- Paper + Software\n",
    "\n",
    "- Paper: \n",
    "  - Introduction\n",
    "  - The NPN Construction\n",
    "  - **Dataset**\n",
    "    - **Corpus Gathering and Cleaning**\n",
    "    - Near Minimal Pairs\n",
    "    - **Train/Test Split**\n",
    "  - **Experiment 1: Constructions vs. Distractors**\n",
    "    - **Methodology**\n",
    "    - Results\n",
    "  - **Experiment 2: Perturbing Word Order**\n",
    "    - Results\n",
    "    - Analysis\n",
    "  - **Experiment 3: Semantic Disambiguation**\n",
    "    - NtoN Subtypes\n",
    "    - **Methodology**\n",
    "    - Results\n",
    "  - Related Work\n",
    "  - Conclusion\n",
    "  - Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b3e28",
   "metadata": {},
   "source": [
    "## La pipeline\n",
    "\n",
    "Creare un dataset **>** \n",
    "Annotare il dataset **>** \n",
    "Estrarre da BERT i vettori contestuali **>** \n",
    "Allenare un classificatore su una porzione di dataset **>** \n",
    "Testare il classificatore su una porzione diversa di dati **>**\n",
    "Analizzare gli errori\n",
    "\n",
    "### Cosa ci aspettiamo?\n",
    "\n",
    "- Dataset\n",
    "  - In che formato?\n",
    "  - Quali informazioni?\n",
    "- Vettori\n",
    "  - Che tipo di file?\n",
    "  - Quanti vettori?\n",
    "- Classificatore\n",
    "  - Input? Output?\n",
    "- Predizioni\n",
    "  - In che formato?\n",
    "  - Come valutiamo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed3618",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "### (3.1) Corpus Gathering and Cleaning\n",
    "\n",
    "> First, we use a simple **pattern matching** query to extract instances of the sequence \n",
    "Noun + “to” + Noun from COCA. We extract the examples from the corpus in a **fixed window of \n",
    "+/- 50 tokens from the construction**\n",
    "\n",
    "> then used Stanza (Qi et al., 2020) to **segment the results into sentences** and extract the sentences which contained NtoNs.\n",
    "\n",
    "> We automatically **exclude sentences which contained “from”** preceding the construction\n",
    "\n",
    "> we then manually clean the data, removing sentences that were either **too short (<5 tokens)** or \n",
    "contained **too many typos**\n",
    "\n",
    "> We **annotate** all instances of the construction for their semantic subtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8d5f8",
   "metadata": {},
   "source": [
    "####  More info\n",
    "\n",
    "> **double annotate** roughly 25% of the dataset, achieving an agreement of 84% and a **Cohen’s kappa** \n",
    "value of .754 between the two annotators, indicating strong agreement. \n",
    "The final dataset has 6599 instances of NtoN, of which 1885 were double annotated.\n",
    "\n",
    "> In total, we collect 456 total instances of NtoN distractors from COCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68405c9e",
   "metadata": {},
   "source": [
    "### (3.3) Train/Test Split\n",
    "\n",
    "> we artificially shrink the dataset by **randomly sampling 20 sentences** for each noun lemma \n",
    "which occurs more than 20 times\n",
    "\n",
    "> we generate **random train/test splits based on lemma of the noun** in the NtoN, meaning\n",
    "that there are no lemmas that are seen in both the training set and the testing set.\n",
    "\n",
    "> We take **80 percent of the NtoN distractor patterns for training and withhold twenty percent**. \n",
    "We take a **similar number of NtoN constructions** for training and then test on the remainder, \n",
    "ensuring **training sets are balanced between constructions and distractors**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12217ed1",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "> We probe the ability for BERT to distinguish natural instances of the NtoN construction from natural\n",
    "examples of the NtoN distractor pattern\n",
    "\n",
    ">  providing two baseline systems which give perspective on performance based on lexical cues: \n",
    "a control classifier and a non-contextual baseline based on GloVe embeddings\n",
    "\t- training a linear classifier on GloVe embeddings for the nouns in the construction as input.\n",
    "\n",
    "> we train a separate probe based on embeddings from each layer of BERT and track\n",
    "performance across layers. We use the BERT-base-cased model, available through the Huggingface\n",
    "transformers library, and choose logistic regression as our linear classification architecture\n",
    "\n",
    "> For all experiments and data settings, we run probes with 5 random seeds and report the\n",
    "average results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d628e3c",
   "metadata": {},
   "source": [
    "## Esperimento 2\n",
    "\n",
    "> we manipulate the test set of the probe by creating 4 perturbed orderings\n",
    "of each test example sentence: PNN, PN, NNP, NP. \n",
    "\n",
    "> Crucially, we do not retrain the linear probe on this perturbed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f180eb",
   "metadata": {},
   "source": [
    "## Esperimento 3\n",
    "\n",
    "> we train a classifier to distinguish semantic subtypes of NtoN. [...] We also \n",
    "include examples of the NtoN distractor patterns which are not examples of the construction.\n",
    "\n",
    "> we train control classifiers with a random label assigned to each lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb8fae",
   "metadata": {},
   "source": [
    "## La pipeline -- dopo aver letto il paper\n",
    "\n",
    "Selezionare contesti dal COCA \n",
    "\n",
    "**>** \n",
    "\n",
    "Segmentare in frasi \n",
    "\n",
    "**>** \n",
    "\n",
    "Filtrare frasi per tenere solo istanze con NtoN, non precedute da `from`, lunghe almeno 5 token \n",
    "\n",
    "**>** \n",
    "\n",
    "Annotare ogni istanza del dataset come `distractor` o uno dei tipi semantici della costruzione \n",
    "\n",
    "**>** \n",
    "\n",
    "Per ogni elemento del dataset, estrarre da BERT-base-cased 12 vettori corrispondenti alla testa\n",
    "della costruzione (preposizione) più un vettore di GloVe (corrispondente al lemma del NOUN) \n",
    "\n",
    "**>** \n",
    "\n",
    "Per 5 volte, scegliere una porzione casuale di dataset:\n",
    "\t- allenare un classificatore lineare sui vettori GloVe\n",
    "\t- per ogni layer di BERT, allenare un classificatore lineare\n",
    "\n",
    "**>** \n",
    "\n",
    "Calcolare la media dell'accuratezza per ogni tipo di embedding\n",
    "\n",
    "**>** \n",
    "\n",
    "Plottare i risultati"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
